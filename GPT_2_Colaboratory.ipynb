{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2 Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sedmegreenaway/gpt-2-colaboratory/blob/master/GPT_2_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwndcuIEPHxu",
        "colab_type": "text"
      },
      "source": [
        "# Setup\n",
        "The following cell sets up everything needed for GPT-2 to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HguYF0VvCws3",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "!mkdir gpt-2\n",
        "!git clone https://github.com/openai/gpt-2.git gpt-2\n",
        "!pip3 install tensorflow-gpu==1.14\n",
        "!pip3 install -r gpt-2/requirements.txt\n",
        "!python3 gpt-2/download_model.py 124M\n",
        "!python3 gpt-2/download_model.py 355M\n",
        "!python3 gpt-2/download_model.py 774M\n",
        "!python3 gpt-2/download_model.py 1558M\n",
        "!cp gpt-2/src/encoder.py ./encoder.py\n",
        "!cp gpt-2/src/sample.py ./sample.py\n",
        "!cp gpt-2/src/model.py ./model.py\n",
        "!rm -rf gpt-2/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7pboNR-Psot",
        "colab_type": "text"
      },
      "source": [
        "# Runner\n",
        "While you can just use commands to run python files retrieved from git during setup, it's easier to do it via a cell, because the cell can have forms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgW1qVtyP1yE",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Interactive Conditional Samples\n",
        "import fire\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import model, sample, encoder\n",
        "\n",
        "def interact_model(#@markdown model_name: String, which model to use\n",
        "model_name = '124M', #@param [\"'124M',\", \"'355M',\", \"'774M',\", \"'1558M',\"] {type:\"raw\"}\n",
        "    #@markdown seed: Integer seed for random number generators, fix seed to reproduce results\n",
        "    seed=None,#@param\n",
        "    #@markdown nsamples: Number of samples to return total\n",
        "    nsamples=1,#@param\n",
        "    #@markdown batch_size: Number of batches (only affects speed/memory).  Must divide nsamples.\n",
        "    batch_size=1,#@param\n",
        "    #@markdown length: Number of tokens in generated text, if None (default), is determined by model hyperparameters\n",
        "    length=None,#@param\n",
        "    #@markdown temperature: Float value controlling randomness in boltzmann distribution. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.\n",
        "    temperature=1,#@param\n",
        "    #@markdown top_k: Integer value controlling diversity. 1 means only 1 word is considered for each step (token), resulting in deterministic completions, while 40 means 40 words are considered at each step. 0 (default) is a special setting meaning no restrictions. 40 generally is a good value.\n",
        "    top_k=40,#@param\n",
        "    top_p=1,\n",
        "    #@markdown models_dir: path to parent folder containing model subfolders (i.e. contains the <model_name> folder)\n",
        "    models_dir='models',#@param\n",
        "):\n",
        "    \"\"\"\n",
        "    Interactively run the model\n",
        "    :model_name=124M : String, which model to use\n",
        "    :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
        "     results\n",
        "    :nsamples=1 : Number of samples to return total\n",
        "    :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
        "    :length=None : Number of tokens in generated text, if None (default), is\n",
        "     determined by model hyperparameters\n",
        "    :temperature=1 : Float value controlling randomness in boltzmann\n",
        "     distribution. Lower temperature results in less random completions. As the\n",
        "     temperature approaches zero, the model will become deterministic and\n",
        "     repetitive. Higher temperature results in more random completions.\n",
        "    :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
        "     considered for each step (token), resulting in deterministic completions,\n",
        "     while 40 means 40 words are considered at each step. 0 (default) is a\n",
        "     special setting meaning no restrictions. 40 generally is a good value.\n",
        "     :models_dir : path to parent folder containing model subfolders\n",
        "     (i.e. contains the <model_name> folder)\n",
        "    \"\"\"\n",
        "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
        "    if batch_size is None:\n",
        "        batch_size = 1\n",
        "    assert nsamples % batch_size == 0\n",
        "\n",
        "    enc = encoder.get_encoder(model_name, models_dir)\n",
        "    hparams = model.default_hparams()\n",
        "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
        "        hparams.override_from_dict(json.load(f))\n",
        "\n",
        "    if length is None:\n",
        "        length = hparams.n_ctx // 2\n",
        "    elif length > hparams.n_ctx:\n",
        "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "    with tf.Session(graph=tf.Graph()) as sess:\n",
        "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "        np.random.seed(seed)\n",
        "        tf.set_random_seed(seed)\n",
        "        output = sample.sample_sequence(\n",
        "            hparams=hparams, length=length,\n",
        "            context=context,\n",
        "            batch_size=batch_size,\n",
        "            temperature=temperature, top_k=top_k, top_p=top_p\n",
        "        )\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
        "        saver.restore(sess, ckpt)\n",
        "\n",
        "        #@markdown interactive: Do you want to be able to provide additional model prompts after the first?\n",
        "        interactive = False #@param {type:\"boolean\"}\n",
        "        has_given_response = False\n",
        "\n",
        "        while ((not has_given_response) or interactive):\n",
        "            if (has_given_response and interactive):\n",
        "                raw_text = input(\"Model prompt >>> \")\n",
        "                while not raw_text:\n",
        "                    print('Prompt should not be empty!')\n",
        "                    raw_text = input(\"Model prompt >>> \")\n",
        "            has_given_response = True\n",
        "            #@markdown raw_text: Model prompt\n",
        "            raw_text = \"Hello.\" #@param {type:\"string\"}\n",
        "            if not raw_text:\n",
        "                print('Prompt should not be empty!')\n",
        "                sys.exit()\n",
        "            context_tokens = enc.encode(raw_text)\n",
        "            generated = 0\n",
        "            for _ in range(nsamples // batch_size):\n",
        "                out = sess.run(output, feed_dict={\n",
        "                    context: [context_tokens for _ in range(batch_size)]\n",
        "                })[:, len(context_tokens):]\n",
        "                for i in range(batch_size):\n",
        "                    generated += 1\n",
        "                    text = enc.decode(out[i])\n",
        "                    print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
        "                    print(text)\n",
        "            print(\"=\" * 80)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    fire.Fire(interact_model)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}